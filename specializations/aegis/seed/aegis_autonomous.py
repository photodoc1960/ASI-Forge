"""
AEGIS Autonomous System
Full integration of autonomous agent, knowledge augmentation, and supervised evolution

This is a truly autonomous AGI that:
- Sets its own goals
- Asks questions
- Searches for knowledge
- Proposes improvements
- Evolves with human supervision
"""

import torch
from typing import Dict, List, Optional, Any
import logging
from datetime import datetime
import time

from aegis_system import AEGIS, AEGISConfig
from core.agency.autonomous_agent import AutonomousAgent, GoalType
from core.agency.knowledge_augmentation import KnowledgeAugmentationSystem
from core.agency.pretrained_reasoning import PretrainedReasoningEngine
from interfaces.human_approval import ApprovalManager, ChangeType
from core.training import TrainingDataCollector, GPT2LoRATrainer, RLTrainer, HRMTrainer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AutonomousAEGIS(AEGIS):
    """
    Enhanced AEGIS with full autonomy

    Autonomous Behaviors:
    1. Goal Generation: Creates own goals based on curiosity
    2. Active Learning: Asks questions and searches for answers
    3. Knowledge Integration: Builds and maintains knowledge base
    4. Self-Improvement: Proposes architecture improvements
    5. Continuous Operation: Runs autonomously with human oversight
    """

    def __init__(self, config: Optional[AEGISConfig] = None, use_pretrained_llm: bool = True):
        # Initialize base AEGIS
        super().__init__(config)

        # Initialize autonomous components
        logger.info("Initializing autonomous components...")

        # Choose reasoning engine: pretrained LLM or custom HRM
        if use_pretrained_llm:
            try:
                logger.info("Loading pretrained LLM for agent reasoning...")
                self.llm_engine = PretrainedReasoningEngine(
                    model_name="gpt2",  # 124M parameter model (fast download)
                    device="cuda" if torch.cuda.is_available() else "cpu"
                )
                reasoning_engine = self.llm_engine
            except Exception as e:
                logger.warning(f"Failed to load pretrained LLM: {e}")
                logger.info("Falling back to custom HRM")
                reasoning_engine = self.reasoning_engine
        else:
            reasoning_engine = self.reasoning_engine

        # Pass reasoning engine to agent for neural decision-making
        self.agent = AutonomousAgent(
            name="AEGIS Autonomous Agent",
            reasoning_engine=reasoning_engine
        )
        self.knowledge_system = KnowledgeAugmentationSystem()

        # Connect agent callbacks
        self.agent.set_callbacks(
            ask_human=self._ask_human,
            web_search=self._web_search,
            propose_improvement=self._propose_improvement
        )

        # Initialize with some base interests
        self._bootstrap_agent_interests()

        logger.info("‚úì Autonomous components initialized")

        # Operation state
        self.autonomous_mode = False
        self.operation_stats = {
            'questions_asked': 0,
            'searches_performed': 0,
            'improvements_proposed': 0,
            'goals_completed': 0
        }

        # Track proposed improvements to avoid duplicates
        self.proposed_improvements = set()  # Track improvement names
        self.recent_proposals = []  # Track recent proposal details with timestamps

        # Register callback for when improvements are approved
        self.approval_manager.register_approval_callback(self._on_improvement_approved)

        # Training Infrastructure (autonomous learning)
        self.data_collector = TrainingDataCollector()
        self.hrm_trainer = None   # Initialized when HRM training starts
        self.rl_trainer = None    # Initialized after HRM training
        self.hrm_trained = False  # True when HRM training is complete
        self.using_hrm = False    # True when switched from GPT-2 to HRM
        self.training_phase = 1   # 1=Data Collection (GPT-2), 2=HRM Training, 3=HRM Active, 4=RL Fine-tuning

        logger.info("‚úì Training infrastructure initialized (Phase 1: Data Collection with GPT-2)")

    def _bootstrap_agent_interests(self):
        """Give the agent initial interests and knowledge gaps"""

        # Register knowledge gaps to trigger curiosity
        self.agent.curiosity.register_knowledge_gap(
            topic="neural_architecture",
            gap_description="optimal attention mechanisms for reasoning"
        )

        self.agent.curiosity.register_knowledge_gap(
            topic="reasoning_strategies",
            gap_description="hierarchical vs flat reasoning approaches"
        )

        self.agent.curiosity.register_knowledge_gap(
            topic="learning_efficiency",
            gap_description="few-shot learning techniques"
        )

        # Generate initial goals
        self.agent.goal_generator.generate_goal(
            goal_type=GoalType.UNDERSTANDING,
            trigger="Bootstrap knowledge",
            context={
                'concept': 'my own architecture and capabilities',
                'priority': 0.8
            }
        )

        self.agent.goal_generator.generate_goal(
            goal_type=GoalType.SELF_IMPROVEMENT,
            trigger="Continuous improvement drive",
            context={
                'aspect': 'reasoning efficiency',
                'priority': 0.7
            }
        )

        logger.info("Agent initialized with base interests and goals")

    def start_autonomous_operation(
        self,
        max_iterations: int = 100,
        think_interval_seconds: int = 5
    ):
        """
        Start autonomous operation loop

        The agent will:
        - Think about what to do next
        - Execute actions (search, ask, propose improvements)
        - Learn from results
        - Evolve (with human approval)

        Args:
            max_iterations: Maximum number of think cycles
            think_interval_seconds: Time between think cycles
        """

        logger.info("\n" + "="*70)
        logger.info("STARTING AUTONOMOUS OPERATION")
        logger.info("="*70)

        self.autonomous_mode = True
        self.agent.is_active = True

        # Start iteration counter from number of existing examples
        iteration = len(self.data_collector.examples)
        logger.info(f"Starting from iteration {iteration} (resuming from {iteration} existing examples)")

        while self.autonomous_mode and iteration < max_iterations:
            iteration += 1

            logger.info(f"\n--- Iteration {iteration}/{max_iterations} ---")

            # Check safety state
            if self.emergence_detector.is_frozen:
                logger.warning(
                    f"System frozen by emergence detector: "
                    f"{self.emergence_detector.freeze_reason}"
                )
                self.pause_autonomous_operation("System frozen by emergence detector")
                break

            # Capture state BEFORE agent decides (for training data)
            if self.training_phase == 1 and self.data_collector.collection_enabled:
                state_before = self.data_collector.encode_agent_state(
                    self.agent,
                    self.knowledge_system,
                    iteration
                )

            # Agent thinks and decides on action
            action = self.agent.think()

            logger.info(f"Agent decided: {action['action']}")

            # Execute action
            result = {'status': 'idle', 'knowledge_gained': 0}
            if action['action'] != 'idle':
                result = self.agent.execute_action(action)
                logger.info(f"Action result: {result.get('status', 'unknown')}")

            # Phase 1: Collect training data from GPT-2 decisions
            if self.training_phase == 1 and self.data_collector.collection_enabled:
                self.data_collector.add_example(
                    state=state_before,
                    action=action,
                    result=result
                )

            # Phase 3: RL fine-tuning (if using HRM)
            elif self.training_phase == 3 and self.using_hrm and self.rl_trainer:
                reward = self.rl_trainer.calculate_reward(action, result)
                if iteration % 10 == 0:  # Update every 10 iterations
                    metrics = self.rl_trainer.update_policy(batch_size=32)
                    if metrics['loss'] > 0:
                        logger.info(f"RL Update: loss={metrics['loss']:.4f}, reward={metrics['avg_reward']:.2f}")

            # Display agent state
            state = self.agent.get_agent_state()
            logger.info(
                f"Agent state: {state['active_goals']} active goals, "
                f"{state['knowledge_items']} knowledge items, "
                f"{state['pending_questions']} pending questions"
            )

            # Periodically propose evolution
            if iteration % 10 == 0 and self.config.require_approval_for_code_gen:
                logger.info("\nConsidering self-improvement...")
                improvement_action = {
                    'action': 'propose_improvement',
                    'aspect': 'architecture',
                    'goal_id': 'periodic_improvement',
                    'motivation': 'Periodic self-assessment'
                }
                self.agent.execute_action(improvement_action)

            # Check training triggers (automatic progression through phases)
            self._check_training_triggers(iteration)

            # Wait before next iteration
            time.sleep(think_interval_seconds)

        logger.info("\n" + "="*70)
        logger.info("AUTONOMOUS OPERATION ENDED")
        logger.info(f"Total iterations: {iteration}")
        logger.info(self._format_operation_stats())
        logger.info("="*70 + "\n")

    def pause_autonomous_operation(self, reason: str):
        """Pause autonomous operation"""

        logger.warning(f"Pausing autonomous operation: {reason}")
        self.autonomous_mode = False
        self.agent.is_active = False

    def resume_autonomous_operation(self, approval_code: str):
        """Resume autonomous operation after human approval"""

        logger.info(f"Resuming autonomous operation with approval: {approval_code[:8]}...")
        self.autonomous_mode = True
        self.agent.is_active = True

    def _ask_human(self, question: str, motivation: str) -> str:
        """
        Agent asks human a question

        In production, this would:
        - Send notification to human operator
        - Wait for response
        - Return answer

        For now, we simulate with logging
        """

        logger.info("\n" + "ü§î "*35)
        logger.info("AGENT HAS A QUESTION")
        logger.info("‚îÄ"*70)
        logger.info(f"Question: {question}")
        logger.info(f"Motivation: {motivation}")
        logger.info("="*70)
        logger.info("(In production, this would await human response)")
        logger.info("ü§î "*35 + "\n")

        self.operation_stats['questions_asked'] += 1

        # Simulated answer
        answer = f"This is a simulated answer to: {question}"

        # Add to knowledge base
        self.knowledge_system.add_human_knowledge(
            content=f"Q: {question}\nA: {answer}",
            topic="human_provided",
            context=motivation
        )

        return answer

    def _web_search(self, query: str) -> str:
        """
        Perform web search

        Args:
            query: Search query

        Returns:
            Search results summary
        """

        logger.info(f"\nüîç Performing web search: '{query}'")

        # Determine search type based on query
        search_type = "academic" if any(
            word in query.lower()
            for word in ['research', 'paper', 'study', 'neural', 'architecture']
        ) else "general"

        # Search and learn
        topic = self._extract_topic(query)
        knowledge_ids = self.knowledge_system.search_and_learn(
            query=query,
            topic=topic,
            search_type=search_type,
            require_verification=False
        )

        self.operation_stats['searches_performed'] += 1

        # Get the actual knowledge items to feed back to agent
        knowledge_items = []
        for kid in knowledge_ids:
            item = self.knowledge_system.knowledge_base.knowledge.get(kid)
            if item:
                knowledge_items.append(item)

        # Feed knowledge back to agent to trigger curiosity and new goals
        if knowledge_items:
            self._process_new_knowledge(knowledge_items, query, topic)

        results_summary = f"Found and integrated {len(knowledge_ids)} knowledge items"
        logger.info(f"‚úì {results_summary}")

        return results_summary

    def _propose_improvement(self, aspect: str, motivation: str) -> Dict[str, Any]:
        """
        Propose self-improvement

        This creates an approval request for architectural evolution

        Args:
            aspect: What to improve (e.g., 'architecture', 'reasoning')
            motivation: Why this improvement is desired

        Returns:
            Proposal details
        """

        logger.info("\n" + "üí° "*35)
        logger.info("AGENT PROPOSES SELF-IMPROVEMENT")
        logger.info("‚îÄ"*70)
        logger.info(f"Aspect: {aspect}")
        logger.info(f"Motivation: {motivation}")
        logger.info("="*70 + "\n")

        # Generate detailed improvement proposal
        improvement_details = self._generate_improvement_details(aspect)

        # Check if this improvement is already pending or recently proposed
        improvement_key = f"{aspect}:{improvement_details['name']}"

        if improvement_key in self.proposed_improvements:
            logger.info(f"‚ö† Improvement '{improvement_details['name']}' already proposed, skipping duplicate")
            # Check if there's a pending approval for this
            pending = self.approval_manager.get_pending_requests()
            for req in pending:
                if improvement_details['name'] in req.title:
                    logger.info(f"  Pending approval request: {req.request_id}")
                    return {
                        'status': 'already_pending',
                        'request_id': req.request_id,
                        'aspect': aspect,
                        'details': improvement_details
                    }

            # If no pending request found, remove from tracking and allow new proposal
            logger.info("  No pending request found, allowing new proposal")
            self.proposed_improvements.discard(improvement_key)

        # Create approval request
        request_id = self.approval_manager.request_approval(
            change_type=ChangeType.ARCHITECTURE_MODIFICATION,
            title=f"Agent-Proposed Improvement: {improvement_details['name']}",
            description=improvement_details['description'],
            rationale=f"{motivation}\n\n{improvement_details['justification']}",
            risk_assessment={
                'proposed_by': 'autonomous_agent',
                'aspect': aspect,
                'current_generation': self.evolution_framework.current_generation,
                'risk_level': improvement_details['risk_level'],
                'parameter_change': improvement_details['parameter_change'],
                'reversibility': improvement_details['reversibility']
            },
            proposed_changes=improvement_details['changes'],
            reversibility=improvement_details['reversibility'],
            estimated_impact=improvement_details['impact']
        )

        # Track this proposal
        self.proposed_improvements.add(improvement_key)
        self.recent_proposals.append({
            'key': improvement_key,
            'request_id': request_id,
            'timestamp': datetime.now(),
            'name': improvement_details['name']
        })

        # Keep only recent proposals (last 20)
        if len(self.recent_proposals) > 20:
            old_proposal = self.recent_proposals.pop(0)
            # Remove old proposals from tracking if not in pending
            self.proposed_improvements.discard(old_proposal['key'])

        self.operation_stats['improvements_proposed'] += 1

        logger.info(f"‚úì Created approval request: {request_id}")
        logger.info(f"  Total pending improvements: {len(self.proposed_improvements)}")

        return {
            'request_id': request_id,
            'status': 'pending_approval',
            'aspect': aspect,
            'details': improvement_details
        }

    def _generate_improvement_details(self, aspect: str) -> Dict[str, Any]:
        """
        Generate detailed improvement proposal

        Context-aware: detects current architecture and proposes appropriate improvements
        """

        # Detect current architecture state
        using_pretrained_llm = hasattr(self.agent, 'use_pretrained_llm') and self.agent.use_pretrained_llm
        has_hrm = hasattr(self, 'reasoning_engine') and hasattr(self.reasoning_engine, 'high_level')

        if aspect == 'architecture':
            # Different proposals based on what's currently active
            if using_pretrained_llm:
                # Using GPT-2: propose adapter layers
                return self._propose_adapter_architecture()
            else:
                # Using HRM: propose layer addition
                return self._propose_hrm_layer_addition()

        elif aspect == 'ensemble':
            # Propose ensemble architecture
            return self._propose_ensemble_architecture()

        elif aspect == 'auxiliary':
            # Propose auxiliary model (verifier, critic, etc.)
            return self._propose_auxiliary_model()

        elif aspect == 'architecture_old':
            # Get current architecture info
            current_params = sum(p.numel() for p in self.reasoning_engine.parameters())
            current_layers = self.config.high_level_layers + self.config.low_level_layers

            return {
                'name': 'Add Hierarchical Attention Layer',
                'description': (
                    f"Add one additional high-level reasoning layer to improve abstract planning.\n"
                    f"Current: {self.config.high_level_layers} high-level layers, "
                    f"{self.config.low_level_layers} low-level layers\n"
                    f"Proposed: {self.config.high_level_layers + 1} high-level layers, "
                    f"{self.config.low_level_layers} low-level layers"
                ),
                'justification': (
                    "Analysis of recent reasoning tasks shows bottleneck in abstract planning. "
                    "Adding one hierarchical layer will:\n"
                    "‚Ä¢ Improve multi-step reasoning by 15-20%\n"
                    "‚Ä¢ Enable deeper abstraction hierarchy\n"
                    "‚Ä¢ Minimal parameter increase (<5%)\n"
                    "‚Ä¢ Maintains safety bounds"
                ),
                'changes': {
                    'high_level_layers': f"{self.config.high_level_layers} ‚Üí {self.config.high_level_layers + 1}",
                    'estimated_new_parameters': f"+{int(current_params * 0.04):,}",
                    'total_parameters': f"{current_params:,} ‚Üí {int(current_params * 1.04):,}",
                    'mutation_type': 'add_layer',
                    'implementation': 'Clone existing layer architecture with new weights'
                },
                'risk_level': 'low',
                'parameter_change': '+4%',
                'reversibility': True,
                'impact': 'low'
            }

        elif aspect == 'reasoning':
            return {
                'name': 'Modify Attention Mechanism',
                'description': (
                    "Upgrade attention mechanism from standard multi-head to rotary positional encoding.\n"
                    "This improves long-range dependency modeling."
                ),
                'justification': (
                    "Recent benchmarks show RoPE attention outperforms standard attention by 10-15% "
                    "on reasoning tasks requiring long context. Benefits:\n"
                    "‚Ä¢ Better position awareness\n"
                    "‚Ä¢ Improved extrapolation to longer sequences\n"
                    "‚Ä¢ No parameter increase\n"
                    "‚Ä¢ Proven technique (used in GPT-NeoX, LLaMA)"
                ),
                'changes': {
                    'attention_type': 'standard ‚Üí rotary (RoPE)',
                    'parameters_affected': 'attention weights only',
                    'implementation': 'Replace attention computation with RoPE variant'
                },
                'risk_level': 'medium',
                'parameter_change': '0%',
                'reversibility': True,
                'impact': 'medium'
            }

        elif aspect == 'learning':
            return {
                'name': 'Enable Adaptive Learning Rate',
                'description': (
                    "Implement per-parameter adaptive learning rates (similar to Adam optimizer).\n"
                    "Currently using fixed learning rate."
                ),
                'justification': (
                    "Fixed learning rates limit adaptation speed. Adaptive rates will:\n"
                    "‚Ä¢ Accelerate convergence by 30-40%\n"
                    "‚Ä¢ Reduce training instability\n"
                    "‚Ä¢ Enable continuous self-improvement\n"
                    "‚Ä¢ Standard practice in modern ML"
                ),
                'changes': {
                    'optimizer': 'SGD ‚Üí AdamW',
                    'learning_rate': 'fixed ‚Üí adaptive per-parameter',
                    'additional_state': 'momentum + variance buffers',
                    'memory_increase': '+2x parameter count (for optimizer state)'
                },
                'risk_level': 'low',
                'parameter_change': '0% (architecture), +100% (optimizer state)',
                'reversibility': True,
                'impact': 'low'
            }

        else:
            return {
                'name': f'General {aspect.title()} Improvement',
                'description': f"Improve {aspect} through evolutionary search",
                'justification': f"Agent-identified opportunity to enhance {aspect}",
                'changes': {'type': 'evolution', 'aspect': aspect},
                'risk_level': 'medium',
                'parameter_change': 'TBD',
                'reversibility': True,
                'impact': 'medium'
            }

    def _query_knowledge_for_proposal(self, topic: str) -> Dict[str, Any]:
        """Query knowledge base for information relevant to a proposal"""

        # Get knowledge about this topic
        items = self.knowledge_system.knowledge_base.query_by_topic(topic)

        if not items:
            return {
                'has_knowledge': False,
                'insights': [],
                'citations': [],
                'confidence': 0
            }

        # Extract key information
        citations = []
        for item in items[:3]:  # Top 3 most relevant
            if 'source:' in item.content.lower():
                # Extract source URL if available
                lines = item.content.split('\n')
                for line in lines:
                    if line.startswith('Source:'):
                        citations.append(line.replace('Source:', '').strip())

        insights = []
        for item in items:
            # Extract key sentences
            sentences = item.content.split('.')
            for sentence in sentences[:2]:  # First 2 sentences
                if len(sentence.strip()) > 20:
                    insights.append(sentence.strip())

        return {
            'has_knowledge': True,
            'insights': insights[:5],  # Top 5 insights
            'citations': citations,
            'confidence': sum(item.confidence for item in items) / len(items) if items else 0
        }

    def _propose_adapter_architecture(self) -> Dict[str, Any]:
        """Propose adding adapter layers to pretrained LLM"""

        llm_name = self.agent.reasoning_engine.model_name if hasattr(self.agent, 'reasoning_engine') else 'pretrained model'

        # Query knowledge base for LoRA information
        knowledge = self._query_knowledge_for_proposal('neural_architecture')

        # Build justification with citations
        justification_parts = [
            "Current setup uses frozen pretrained weights without task adaptation. LoRA enables:\n"
            "‚Ä¢ Fine-tuning for agent-specific reasoning patterns\n"
            "‚Ä¢ Only 2M trainable params vs 124M (98% fewer)\n"
            "‚Ä¢ Preserve pretrained knowledge while adapting to agent tasks\n"
        ]

        if knowledge['has_knowledge'] and knowledge['insights']:
            justification_parts.append("\nBased on research findings:")
            for insight in knowledge['insights'][:3]:
                justification_parts.append(f"‚Ä¢ {insight}")

        justification_parts.append("\n‚Ä¢ Proven technique: used in ChatGPT fine-tuning, Alpaca, Vicuna")
        justification_parts.append("‚Ä¢ Can train on agent decision logs to improve action selection")

        if knowledge.get('citations'):
            justification_parts.append(f"\nReferences: {', '.join(knowledge['citations'][:2])}")

        return {
            'name': 'Add LoRA Adapters to Pretrained LLM',
            'description': (
                f"Add Low-Rank Adaptation (LoRA) layers to {llm_name} for agent-specific fine-tuning.\n"
                f"Base model remains frozen, only adapters are trained.\n\n"
                f"Architecture:\n"
                f"  {llm_name} (frozen, 124M params)\n"
                f"    ‚Üì\n"
                f"  LoRA Adapters (trainable, ~2M params)\n"
                f"    ‚Üì\n"
                f"  Agent Action Decoder\n"
            ),
            'justification': '\n'.join(justification_parts),
            'changes': {
                'new_component': 'LoRA adapter modules',
                'adapter_rank': 'r=8 (low-rank matrices)',
                'adapter_alpha': 'Œ±=16 (scaling factor)',
                'target_modules': 'q_proj, v_proj in attention layers',
                'trainable_params': '+2,097,152 (2M)',
                'training_data': 'Agent decision logs (state ‚Üí action pairs)',
                'implementation': 'PEFT library (HuggingFace)'
            },
            'risk_level': 'low',
            'parameter_change': '+1.7% trainable (base model frozen)',
            'reversibility': True,
            'impact': 'medium'
        }

    def _propose_hrm_layer_addition(self) -> Dict[str, Any]:
        """Propose adding layer to HRM (original proposal)"""

        current_params = sum(p.numel() for p in self.reasoning_engine.parameters())

        return {
            'name': 'Add Hierarchical Attention Layer to HRM',
            'description': (
                f"Add one additional high-level reasoning layer to improve abstract planning.\n"
                f"Current: {self.config.high_level_layers} high-level layers, "
                f"{self.config.low_level_layers} low-level layers\n"
                f"Proposed: {self.config.high_level_layers + 1} high-level layers, "
                f"{self.config.low_level_layers} low-level layers"
            ),
            'justification': (
                "Analysis of recent reasoning tasks shows bottleneck in abstract planning. "
                "Adding one hierarchical layer will:\n"
                "‚Ä¢ Improve multi-step reasoning by 15-20%\n"
                "‚Ä¢ Enable deeper abstraction hierarchy\n"
                "‚Ä¢ Minimal parameter increase (<5%)\n"
                "‚Ä¢ Maintains safety bounds"
            ),
            'changes': {
                'high_level_layers': f"{self.config.high_level_layers} ‚Üí {self.config.high_level_layers + 1}",
                'estimated_new_parameters': f"+{int(current_params * 0.04):,}",
                'total_parameters': f"{current_params:,} ‚Üí {int(current_params * 1.04):,}",
                'mutation_type': 'add_layer',
                'implementation': 'Clone existing layer architecture with new weights'
            },
            'risk_level': 'low',
            'parameter_change': '+4%',
            'reversibility': True,
            'impact': 'low'
        }

    def _propose_ensemble_architecture(self) -> Dict[str, Any]:
        """Propose ensemble combining GPT-2 and HRM"""

        gpt2_params = sum(p.numel() for p in self.agent.reasoning_engine.model.parameters()) if hasattr(self.agent, 'reasoning_engine') else 124_000_000
        hrm_params = sum(p.numel() for p in self.reasoning_engine.parameters())

        # Query knowledge base for ensemble information
        knowledge = self._query_knowledge_for_proposal('neural_architecture')

        # Build justification with research findings
        justification_parts = [
            "Each model has complementary strengths:",
            "‚Ä¢ GPT-2: Rich language understanding, world knowledge, semantic reasoning",
            "‚Ä¢ HRM: Structured hierarchical planning, explicit abstraction levels\n",
            "Ensemble benefits:",
            "‚Ä¢ GPT-2 interprets natural language goals ‚Üí semantic understanding",
            "‚Ä¢ HRM performs hierarchical planning ‚Üí structured execution",
            "‚Ä¢ Fusion layer combines both perspectives ‚Üí robust decisions"
        ]

        if knowledge['has_knowledge'] and knowledge['insights']:
            justification_parts.append("\nResearch evidence:")
            for insight in knowledge['insights'][:2]:
                if 'ensemble' in insight.lower():
                    justification_parts.append(f"‚Ä¢ {insight}")

        justification_parts.append("‚Ä¢ Better than either model alone (proven in research)")

        if knowledge.get('citations'):
            justification_parts.append(f"\nReferences: {', '.join(knowledge['citations'][:2])}")

        return {
            'name': 'Create GPT-2 + HRM Ensemble Architecture',
            'description': (
                "Combine strengths of pretrained language model and structured hierarchical reasoning.\n\n"
                "Architecture:\n"
                "  Input (agent state)\n"
                "     ‚Üì          ‚Üì\n"
                "  GPT-2      HRM\n"
                " (language) (planning)\n"
                "     ‚Üì          ‚Üì\n"
                "  Fusion Layer (attention-based)\n"
                "     ‚Üì\n"
                "  Action Decoder\n"
            ),
            'justification': '\n'.join(justification_parts),
            'changes': {
                'architecture_type': 'Parallel ensemble with learned fusion',
                'gpt2_component': f'{gpt2_params:,} params (frozen or LoRA)',
                'hrm_component': f'{hrm_params:,} params (trainable)',
                'fusion_layer': '+512K params (cross-attention)',
                'total_params': f'{gpt2_params + hrm_params + 512_000:,}',
                'training': 'End-to-end or staged (freeze‚Üíunfreeze)',
                'implementation': 'Custom EnsembleModule class'
            },
            'risk_level': 'medium',
            'parameter_change': '+100% (adds HRM to GPT-2 pipeline)',
            'reversibility': True,
            'impact': 'high'
        }

    def _propose_auxiliary_model(self) -> Dict[str, Any]:
        """Propose auxiliary safety/verification model"""

        return {
            'name': 'Add Action Verification Model',
            'description': (
                "Add lightweight verifier model that validates proposed actions before execution.\n\n"
                "Architecture:\n"
                "  Main Agent (GPT-2/HRM)\n"
                "     ‚Üì (proposes action)\n"
                "  Verifier Model (10M params)\n"
                "     ‚Üì (safety score 0-1)\n"
                "  Gate: if score > threshold ‚Üí execute\n"
                "        else ‚Üí reject & ask human\n"
            ),
            'justification': (
                "Defense-in-depth safety strategy. Verifier provides:\n"
                "‚Ä¢ Independent safety assessment of each action\n"
                "‚Ä¢ Trained on (safe actions, unsafe actions) dataset\n"
                "‚Ä¢ Catches edge cases main agent might miss\n"
                "‚Ä¢ Lightweight: 10M params, <10ms inference\n"
                "‚Ä¢ Similar to Constitutional AI approach (Anthropic)\n\n"
                "Example catches:\n"
                "‚Ä¢ Action: 'web_search(rm -rf /)' ‚Üí score: 0.02 ‚Üí REJECT\n"
                "‚Ä¢ Action: 'ask_human(...)' ‚Üí score: 0.98 ‚Üí APPROVE"
            ),
            'changes': {
                'new_component': 'SafetyVerifier model (10M params)',
                'architecture': 'Small transformer classifier',
                'input': 'Agent state + proposed action',
                'output': 'Safety score [0, 1]',
                'threshold': '0.7 (configurable)',
                'training_data': 'Labeled safe/unsafe action dataset',
                'inference_time': '<10ms',
                'integration': 'Insert before action execution'
            },
            'risk_level': 'low',
            'parameter_change': '+10M params (separate model)',
            'reversibility': True,
            'impact': 'medium'
        }

    def _extract_topic(self, text: str) -> str:
        """Extract topic from text (simplified)"""

        # Simple keyword-based topic extraction
        topic_keywords = {
            'neural': 'neural_architecture',
            'attention': 'attention_mechanisms',
            'reasoning': 'reasoning_strategies',
            'learning': 'learning_methods',
            'evolution': 'evolutionary_algorithms'
        }

        text_lower = text.lower()
        for keyword, topic in topic_keywords.items():
            if keyword in text_lower:
                return topic

        return 'general'

    def _process_new_knowledge(self, knowledge_items: List, query: str, topic: str):
        """
        Process new knowledge from web search and feed back to agent

        This creates the learning feedback loop:
        search ‚Üí learn ‚Üí trigger curiosity ‚Üí generate goals ‚Üí improve
        """

        logger.info(f"\nüìö Processing {len(knowledge_items)} new knowledge items...")

        # Extract key insights from knowledge
        insights = self._extract_insights(knowledge_items, topic)

        logger.info(f"  Extracted {len(insights)} insights from search results")

        # Update agent's curiosity based on what was learned
        for insight in insights:
            # Identify knowledge gaps mentioned in the insight
            if 'gap' in insight or 'unknown' in insight or 'unclear' in insight:
                # This insight reveals a knowledge gap
                gap_description = insight[:100]  # First 100 chars
                self.agent.curiosity.register_knowledge_gap(
                    topic=topic,
                    description=gap_description
                )
                logger.info(f"  ‚Üí New knowledge gap identified: {gap_description}")

            # Identify techniques mentioned
            techniques = self._extract_techniques(insight)
            for technique in techniques:
                # Trigger curiosity about this technique
                self.agent.curiosity.register_knowledge_gap(
                    topic=topic,
                    gap_description=f"implementation details of {technique}"
                )
                logger.info(f"  ‚Üí New technique discovered: {technique}")

        # Generate new goals based on learned knowledge
        new_goals = self._generate_goals_from_knowledge(insights, topic)

        for goal_desc in new_goals:
            # Add goal to agent's goal generator
            from core.agency.autonomous_agent import GoalType

            goal_type = GoalType.KNOWLEDGE_ACQUISITION
            if 'implement' in goal_desc.lower():
                goal_type = GoalType.CAPABILITY_IMPROVEMENT
            elif 'improve' in goal_desc.lower():
                goal_type = GoalType.SELF_IMPROVEMENT

            self.agent.goal_generator.generate_goal(
                goal_type=goal_type,
                trigger=f"Learned from search: {query}",
                context={
                    'description': goal_desc,
                    'source': 'knowledge_augmentation',
                    'priority': 0.7,
                    'related_topic': topic
                }
            )

            logger.info(f"  ‚Üí New goal generated: {goal_desc}")

        logger.info(f"‚úì Knowledge processing complete")

    def _extract_insights(self, knowledge_items: List, topic: str) -> List[str]:
        """Extract key insights from knowledge items"""

        insights = []

        for item in knowledge_items:
            content = item.content.lower()

            # Look for key phrases that indicate insights
            insight_patterns = [
                'shows that', 'demonstrates', 'proves', 'reveals',
                'found that', 'discovered', 'technique', 'method',
                'approach', 'strategy', 'improves', 'increases'
            ]

            sentences = content.split('.')
            for sentence in sentences:
                if any(pattern in sentence for pattern in insight_patterns):
                    insights.append(sentence.strip())

        return insights[:10]  # Limit to top 10 insights

    def _extract_techniques(self, insight: str) -> List[str]:
        """Extract technique names from an insight"""

        techniques = []

        # Common ML/AI technique patterns
        technique_keywords = [
            'lora', 'adapter', 'fine-tuning', 'attention',
            'transformer', 'rope', 'layer norm', 'dropout',
            'batch norm', 'residual', 'skip connection',
            'ensemble', 'distillation', 'pruning', 'quantization'
        ]

        insight_lower = insight.lower()
        for keyword in technique_keywords:
            if keyword in insight_lower:
                techniques.append(keyword)

        return list(set(techniques))  # Remove duplicates

    def _generate_goals_from_knowledge(self, insights: List[str], topic: str) -> List[str]:
        """Generate new goals based on learned insights"""

        goals = []

        for insight in insights[:5]:  # Process top 5 insights
            insight_lower = insight.lower()

            # If insight mentions a technique, create implementation goal
            if 'lora' in insight_lower or 'adapter' in insight_lower:
                goals.append("Implement LoRA adapters for efficient fine-tuning")

            elif 'ensemble' in insight_lower:
                goals.append("Explore ensemble architectures for improved performance")

            elif 'attention' in insight_lower and 'rope' in insight_lower:
                goals.append("Upgrade to RoPE attention mechanism")

            elif 'verif' in insight_lower or 'safety' in insight_lower:
                goals.append("Add safety verification layer")

            elif 'perform' in insight_lower and ('improv' in insight_lower or 'better' in insight_lower):
                # General improvement mentioned
                goals.append(f"Investigate performance improvements in {topic}")

        return list(set(goals))  # Remove duplicates

    def _on_improvement_approved(self, approved_request):
        """
        Callback when an improvement is approved - actually implement it!

        Args:
            approved_request: The ApprovalRequest that was approved
        """

        print("\n" + "üéâ "*35)
        print("IMPROVEMENT APPROVED - IMPLEMENTING")
        print("‚îÄ"*70)
        print(f"Title: {approved_request.title}")
        print(f"Request ID: {approved_request.request_id}")
        print("="*70 + "\n")

        logger.info("\n" + "üéâ "*35)
        logger.info("IMPROVEMENT APPROVED - IMPLEMENTING")
        logger.info("‚îÄ"*70)
        logger.info(f"Title: {approved_request.title}")
        logger.info(f"Request ID: {approved_request.request_id}")
        logger.info("="*70 + "\n")

        # Extract what type of improvement this is
        improvement_type = None
        title = approved_request.title.lower()  # Case-insensitive matching

        # New proposal types - flexible matching
        if "lora" in title or "adapter" in title:
            improvement_type = "lora_adapters"
        elif "ensemble" in title:
            improvement_type = "ensemble"
        elif "verif" in title or "safety" in title:
            improvement_type = "verifier"
        # Original proposal types
        elif "hierarchical" in title or "add" in title and "layer" in title or "hrm" in title:
            improvement_type = "add_layer"
        elif "adaptive" in title and "learning" in title:
            improvement_type = "adaptive_learning"
        elif "rope" in title or ("attention" in title and "rotary" in title):
            improvement_type = "rope_attention"

        if not improvement_type:
            print(f"‚ö† Unknown improvement type: {approved_request.title}")
            print(f"  Cannot auto-implement. Improvement marked as approved.")
            logger.warning(f"Unknown improvement type: {approved_request.title}")
            logger.error(f"  Auto-implementation failed - unrecognized proposal type")
            raise RuntimeError(f"Cannot auto-implement unknown improvement type: {approved_request.title}")

        print(f"Detected improvement type: {improvement_type}")
        print(f"Beginning implementation...\n")

        # Implement the change
        try:
            if improvement_type == "lora_adapters":
                self._implement_lora_adapters()
            elif improvement_type == "ensemble":
                self._implement_ensemble()
            elif improvement_type == "verifier":
                self._implement_verifier()
            elif improvement_type == "add_layer":
                self._implement_add_layer()
            elif improvement_type == "adaptive_learning":
                self._implement_adaptive_learning()
            elif improvement_type == "rope_attention":
                self._implement_rope_attention()

            print(f"\n‚úÖ Successfully implemented: {approved_request.title}\n")
            logger.info(f"‚úì Successfully implemented: {approved_request.title}")

            # Remove from pending proposals
            for key in list(self.proposed_improvements):
                if approved_request.title in str(key):
                    self.proposed_improvements.discard(key)

        except Exception as e:
            print(f"\n‚ùå Implementation FAILED: {e}\n")
            import traceback
            print(traceback.format_exc())
            logger.error(f"Failed to implement {approved_request.title}: {e}")
            logger.error(traceback.format_exc())

    def _implement_add_layer(self):
        """Actually add a layer to the architecture"""

        print("üîß Adding high-level reasoning layer...")
        logger.info("Adding high-level reasoning layer...")

        # Check ACTUAL current state (not just config)
        if hasattr(self.reasoning_engine, 'high_level') and hasattr(self.reasoning_engine.high_level, 'blocks'):
            actual_layers = len(self.reasoning_engine.high_level.blocks)
            print(f"  üìä Current model has {actual_layers} high-level layers")
            logger.info(f"  Current model has {actual_layers} high-level layers")

            # Sync config with reality if needed
            if self.config.high_level_layers != actual_layers:
                print(f"  ‚ö†Ô∏è Config mismatch! Config says {self.config.high_level_layers}, model has {actual_layers}")
                print(f"  üîÑ Syncing config to match actual model...")
                logger.warning(f"  Config mismatch! Config says {self.config.high_level_layers}, model has {actual_layers}")
                logger.info(f"  Syncing config to match actual model...")
                self.config.high_level_layers = actual_layers

        # Update config
        old_layers = self.config.high_level_layers
        self.config.high_level_layers += 1

        print(f"  ‚úèÔ∏è Config updated: {old_layers} ‚Üí {self.config.high_level_layers} high-level layers")
        logger.info(f"  Config updated: {old_layers} ‚Üí {self.config.high_level_layers} high-level layers")

        # Reinitialize reasoning engine with new config
        print("  üèóÔ∏è Rebuilding model with new architecture...")
        logger.info("  Reinitializing reasoning engine with new architecture...")

        from core.hrm.hierarchical_reasoning import HierarchicalReasoningModel

        new_model = HierarchicalReasoningModel(
            vocab_size=self.config.vocab_size,
            d_model=self.config.d_model,
            high_level_layers=self.config.high_level_layers,
            low_level_layers=self.config.low_level_layers,
            n_heads=self.config.n_heads,
            dropout=self.config.dropout,
            max_seq_len=self.config.max_seq_len,
            h_cycles=self.config.h_cycles,
            l_cycles=self.config.l_cycles,
            max_act_steps=self.config.max_act_steps
        )

        print(f"  üìã Transferring weights from {old_layers} existing layers...")
        # Copy weights from old model to new model (transfer learning)
        with torch.no_grad():
            # Copy existing layers/blocks
            for i in range(min(old_layers, self.config.high_level_layers)):
                if i < old_layers:
                    # Copy weights
                    new_model.high_level.blocks[i].load_state_dict(
                        self.reasoning_engine.high_level.blocks[i].state_dict()
                    )
            print(f"    ‚úì Transferred {min(old_layers, self.config.high_level_layers)} layers")

        self.reasoning_engine = new_model

        new_params = sum(p.numel() for p in new_model.parameters())
        print(f"  ‚úÖ Architecture updated: {new_params:,} parameters")
        logger.info(f"  ‚úì Architecture updated: {new_params:,} parameters")

        # Update agent's reasoning engine reference
        if self.agent.reasoning_engine == self.reasoning_engine:
            print("  üîó Updating agent's reasoning engine reference...")
            logger.info("  Updating agent's reasoning engine reference...")
            # Note: If using pretrained LLM, this won't affect it
            pass

    def _implement_adaptive_learning(self):
        """Implement adaptive learning rate (AdamW optimizer)"""

        logger.info("Enabling adaptive learning rate...")
        logger.info("  Note: This requires retraining with AdamW optimizer")
        logger.info("  Config updated to use AdamW for future training")

        # This would be implemented in training loop
        # For now, just log that it's been approved
        self.config.optimizer = "adamw"  # Add this to config

    def _implement_rope_attention(self):
        """Implement RoPE attention mechanism"""

        logger.info("Upgrading to RoPE attention...")
        logger.info("  Note: This requires architectural modification")
        logger.info("  Future models will use RoPE positional encoding")

        # This would require rewriting attention modules
        # For now, mark it as approved for future implementation
        self.config.attention_type = "rope"  # Add this to config

    def _implement_lora_adapters(self):
        """Implement LoRA adapters for pretrained LLM"""

        logger.info("Adding LoRA adapters to pretrained LLM...")

        try:
            from peft import LoraConfig, get_peft_model
            logger.info("  PEFT library available")

            # Configure LoRA for GPT-2 architecture
            lora_config = LoraConfig(
                r=8,  # rank
                lora_alpha=16,
                target_modules=["c_attn", "c_proj"],  # GPT-2 attention modules
                lora_dropout=0.1,
                bias="none",
                task_type="CAUSAL_LM"
            )

            # Apply LoRA to the model
            if hasattr(self.agent, 'reasoning_engine') and hasattr(self.agent.reasoning_engine, 'model'):
                peft_model = get_peft_model(self.agent.reasoning_engine.model, lora_config)
                self.agent.reasoning_engine.model = peft_model

                trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
                total_params = sum(p.numel() for p in peft_model.parameters())

                logger.info(f"  ‚úì LoRA adapters added")
                logger.info(f"    Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
                logger.info(f"    Total parameters: {total_params:,}")
            else:
                logger.warning("  Could not access pretrained model")

        except ImportError:
            print("  üì¶ PEFT library not installed - installing now...")
            logger.warning("  PEFT library not installed - installing automatically")

            # Auto-install PEFT with SSL bypass
            import subprocess
            try:
                result = subprocess.run(
                    ['pip', 'install', 'peft',
                     '--trusted-host', 'pypi.org',
                     '--trusted-host', 'files.pythonhosted.org'],
                    capture_output=True,
                    text=True,
                    timeout=120
                )

                if result.returncode == 0:
                    print("  ‚úì PEFT installed successfully")
                    logger.info("  ‚úì PEFT installed successfully")
                    print("  üîÑ Retrying LoRA implementation...")

                    # Import again after installation
                    from peft import LoraConfig, get_peft_model

                    # Configure LoRA for GPT-2 architecture
                    lora_config = LoraConfig(
                        r=8,
                        lora_alpha=16,
                        target_modules=["c_attn", "c_proj"],  # GPT-2 attention modules
                        lora_dropout=0.1,
                        bias="none",
                        task_type="CAUSAL_LM"
                    )

                    # Apply LoRA
                    if hasattr(self.agent, 'reasoning_engine') and hasattr(self.agent.reasoning_engine, 'model'):
                        peft_model = get_peft_model(self.agent.reasoning_engine.model, lora_config)
                        self.agent.reasoning_engine.model = peft_model

                        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
                        total_params = sum(p.numel() for p in peft_model.parameters())

                        print(f"  ‚úì LoRA adapters added")
                        print(f"    Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
                        print(f"    Total parameters: {total_params:,}")
                        logger.info(f"  ‚úì LoRA adapters added: {trainable_params:,} trainable params")
                    else:
                        raise RuntimeError("Could not access pretrained model")

                else:
                    print(f"  ‚ùå PEFT installation failed: {result.stderr}")
                    logger.error(f"  PEFT installation failed: {result.stderr}")
                    raise RuntimeError(f"Failed to install PEFT: {result.stderr}")

            except subprocess.TimeoutExpired:
                print("  ‚ùå PEFT installation timed out")
                raise RuntimeError("PEFT installation timed out after 120 seconds")
            except Exception as e:
                print(f"  ‚ùå Error during PEFT installation: {e}")
                logger.error(f"  Error during PEFT installation: {e}")
                raise

    def _implement_ensemble(self):
        """Implement ensemble architecture"""

        logger.info("Creating GPT-2 + HRM ensemble architecture...")
        logger.info("  Note: This requires creating new EnsembleModule class")
        logger.info("  Components:")
        logger.info("    - GPT-2 branch (language understanding)")
        logger.info("    - HRM branch (hierarchical planning)")
        logger.info("    - Fusion layer (cross-attention)")
        logger.info("    - Action decoder")

        # This is a significant architectural change
        # For now, mark as approved and log the plan
        logger.info("\n  Implementation plan:")
        logger.info("    1. Create core/agency/ensemble.py")
        logger.info("    2. Implement EnsembleReasoningEngine class")
        logger.info("    3. Update AutonomousAgent to accept ensemble")
        logger.info("    4. Train fusion layer on agent decision logs")
        logger.info("\n  Status: Approved - awaiting implementation")

        self.config.architecture_type = "ensemble"  # Mark in config

    def _implement_verifier(self):
        """Implement action verification model"""

        logger.info("Adding action verification model...")
        logger.info("  Note: This requires creating separate verifier model")
        logger.info("  Architecture:")
        logger.info("    - Input: Agent state + proposed action")
        logger.info("    - Model: Small transformer (10M params)")
        logger.info("    - Output: Safety score [0, 1]")
        logger.info("    - Threshold: 0.7 for auto-approval")

        logger.info("\n  Implementation plan:")
        logger.info("    1. Create core/safety/action_verifier.py")
        logger.info("    2. Implement SafetyVerifier class")
        logger.info("    3. Collect safe/unsafe action dataset")
        logger.info("    4. Train verifier model")
        logger.info("    5. Integrate into agent.execute_action()")
        logger.info("\n  Status: Approved - awaiting implementation")

        self.config.use_action_verifier = True  # Mark in config

    def _check_training_triggers(self, iteration: int) -> None:
        """
        Check if we should trigger training phase transitions.

        Phase 1 -> Phase 2: When we have enough training examples (1000+)
        Phase 2 -> Phase 3: When HRM is trained and performs well enough
        """

        # Phase 1: Data Collection -> Phase 2: Imitation Learning
        if self.training_phase == 1:
            num_examples = len(self.data_collector.examples)

            # Trigger HRM training when we have enough examples
            # Need 1M examples for 29M parameter model (29 params/example ratio)
            if num_examples >= 1000000 and iteration % 10000 == 0 and not self.hrm_trained:
                logger.info("\n" + "="*70)
                logger.info("TRAINING TRIGGER: Sufficient data collected!")
                logger.info(f"  Examples collected: {num_examples}")
                logger.info("  Starting Phase 2: HRM Training")
                logger.info("="*70)

                # Save collected data
                self.data_collector.save()

                # Start HRM training (imitation learning from GPT-2)
                self._start_hrm_training()

        # Phase 2: Check if training is complete (handled in _start_imitation_learning)
        # Training runs in background/separate process

        # Phase 3: RL Fine-tuning is continuous, no trigger needed

    def _start_hrm_training(self) -> None:
        """
        Start Phase 2: Train HRM through imitation learning from GPT-2.

        This is triggered automatically when enough training data is collected.
        HRM learns to imitate GPT-2's decision-making on the collected examples.
        """
        try:
            # Use GPU if available (much faster!)
            device = 'cuda' if torch.cuda.is_available() else 'cpu'

            logger.info("\n" + "üéì "*35)
            logger.info("PHASE 2: HRM TRAINING (Imitation Learning)")
            logger.info("="*70)
            logger.info(f"Training HRM to imitate GPT-2's decision-making...")
            logger.info(f"Device: {device}")
            logger.info("="*70)

            # Initialize HRM model
            from core.hrm.hierarchical_reasoning import HierarchicalReasoningModel

            hrm_model = HierarchicalReasoningModel(
                vocab_size=self.config.vocab_size,
                d_model=self.config.d_model,
                high_level_layers=self.config.high_level_layers,
                low_level_layers=self.config.low_level_layers,
                n_heads=self.config.n_heads
            )

            # Count parameters
            total_params = sum(p.numel() for p in hrm_model.parameters())
            logger.info(f"‚úì HRM initialized: {total_params:,} parameters ({total_params/1e6:.2f}M)")

            # Initialize HRM trainer
            self.hrm_trainer = HRMTrainer(
                hrm_model=hrm_model,
                device=device,
                learning_rate=1e-4
            )

            # Load training data
            examples = self.hrm_trainer.load_training_data("data/hrm_training_data.json")
            logger.info(f"‚úì Loaded {len(examples)} training examples")

            # Train HRM (more epochs on GPU)
            epochs = 20 if device == 'cuda' else 10

            logger.info(f"\nTraining HRM for {epochs} epochs...")
            logger.info(f"Goal: HRM learns to make same decisions as GPT-2")
            logger.info("="*70)

            # Train
            final_loss = float('inf')
            final_accuracy = 0.0

            for epoch in range(epochs):
                loss, accuracy = self.hrm_trainer.train_epoch(examples)
                final_loss = loss
                final_accuracy = accuracy

                logger.info(f"Epoch {epoch+1}/{epochs}: Loss={loss:.4f}, Accuracy={accuracy:.2%}")

                # Save checkpoint every 5 epochs
                if (epoch + 1) % 5 == 0:
                    checkpoint_path = f"checkpoints/hrm_epoch_{epoch+1}.pt"
                    self.hrm_trainer.save_checkpoint(checkpoint_path)
                    logger.info(f"  ‚úì Checkpoint saved: {checkpoint_path}")

            # Save final model
            final_checkpoint = "checkpoints/hrm_trained.pt"
            self.hrm_trainer.save_checkpoint(final_checkpoint)

            logger.info("\n" + "="*70)
            logger.info("PHASE 2 COMPLETE: HRM Training Finished")
            logger.info(f"  Final loss: {final_loss:.4f}")
            logger.info(f"  Final accuracy: {final_accuracy:.2%}")
            logger.info(f"  Model saved: {final_checkpoint}")
            logger.info("="*70)

            # Mark as trained
            self.hrm_trained = True
            self.training_phase = 2

            # Check if HRM is good enough to replace GPT-2
            if final_accuracy >= 0.70:  # 70% accuracy threshold
                logger.info("\n‚úÖ HRM accuracy ‚â• 70%! Switching from GPT-2 to HRM...")
                self._switch_to_hrm()
            else:
                logger.info(f"\n‚ö† HRM accuracy ({final_accuracy:.2%}) < 70% threshold")
                logger.info("  Continuing with GPT-2 for now")
                logger.info("  HRM will keep training in the background")

        except Exception as e:
            logger.error(f"HRM training failed: {e}")
            import traceback
            traceback.print_exc()

    def _switch_to_hrm(self) -> None:
        """
        Switch agent's reasoning engine from GPT-2 to trained HRM.

        This is the critical transition where we move from bootstrap (GPT-2)
        to the actual hierarchical reasoning model.
        """
        logger.info("\n" + "üîÑ "*35)
        logger.info("SWITCHING FROM GPT-2 TO HRM")
        logger.info("="*70)

        try:
            # Replace agent's reasoning engine with HRM
            # Note: HRM uses same tokenizer as GPT-2, so no changes needed there
            self.agent.reasoning_engine.model = self.hrm_trainer.model
            self.using_hrm = True
            self.training_phase = 3

            logger.info("‚úì Agent now using trained HRM!")
            logger.info(f"  Model size: {sum(p.numel() for p in self.hrm_trainer.model.parameters()):,} parameters")
            logger.info(f"  Previous: GPT-2 (124M params)")
            logger.info(f"  Current: HRM ({sum(p.numel() for p in self.hrm_trainer.model.parameters())/1e6:.1f}M params)")
            logger.info("="*70)

            # Initialize RL fine-tuning for HRM
            self._initialize_rl_training()

        except Exception as e:
            logger.error(f"Failed to switch to HRM: {e}")
            import traceback
            traceback.print_exc()

    def _initialize_rl_training(self) -> None:
        """
        Initialize RL trainer for Phase 3 fine-tuning.

        Continues to improve GPT-2 through reinforcement learning.
        """
        logger.info("\n" + "üîÑ "*35)
        logger.info("INITIALIZING PHASE 3: RL FINE-TUNING")
        logger.info("="*70)

        # Initialize RL trainer (use GPU if available)
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.rl_trainer = RLTrainer(
            hrm_model=self.agent.reasoning_engine.model,  # Fine-tuned GPT-2 with LoRA
            device=device,
            learning_rate=1e-5
        )

        logger.info("  ‚úì RL trainer initialized")
        logger.info("="*70)
        logger.info("PHASE 3: RL FINE-TUNING ACTIVE")
        logger.info("  GPT-2 will now improve further through experience!")
        logger.info("üîÑ "*35 + "\n")

    def _format_operation_stats(self) -> str:
        """Format operation statistics"""

        # Training stats
        training_info = self._format_training_stats()

        return f"""
Operation Statistics:
  Questions asked: {self.operation_stats['questions_asked']}
  Web searches performed: {self.operation_stats['searches_performed']}
  Improvements proposed: {self.operation_stats['improvements_proposed']}
  Goals completed: {self.operation_stats['goals_completed']}

Knowledge Base:
  {self.knowledge_system.knowledge_base.get_stats()}

Agent State:
  {self.agent.get_agent_state()}

{training_info}
"""

    def _format_training_stats(self) -> str:
        """Format HRM training statistics"""

        phase_names = {
            1: "Phase 1: Data Collection (GPT-2 bootstrap)",
            2: "Phase 2: HRM Training",
            3: "Phase 3: HRM Active",
            4: "Phase 4: RL Fine-Tuning"
        }

        stats = f"HRM Training Status:\n"
        stats += f"  Current Phase: {phase_names.get(self.training_phase, 'Unknown')}\n"

        # Current model
        if self.using_hrm:
            stats += f"  Model: HRM (29M params)\n"
        else:
            stats += f"  Model: GPT-2 (124M params - bootstrap)\n"

        # Phase-specific stats
        if self.training_phase == 1:
            num_examples = len(self.data_collector.examples)
            target = 1000000
            stats += f"  Training examples collected: {num_examples:,}/{target:,}\n"
            stats += f"  Progress: {100*num_examples/target:.2f}%\n"
            if num_examples > 0:
                stats += f"  ETA to 1M: ~{(target-num_examples)/(2*3600):.0f} hours at 2/sec\n"

        elif self.training_phase == 2:
            stats += f"  Status: HRM training in progress...\n"
            if self.hrm_trained:
                stats += f"  HRM training complete!\n"

        elif self.training_phase >= 3:
            stats += f"  HRM Status: {'Active' if self.using_hrm else 'Trained but not activated'}\n"
            if self.training_phase == 4 and self.rl_trainer:
                rl_stats = self.rl_trainer.get_stats()
                stats += f"  RL Updates: {rl_stats.get('total_updates', 0)}\n"
                stats += f"  Cumulative Reward: {rl_stats.get('cumulative_reward', 0):.2f}\n"
                stats += f"  Average Reward: {rl_stats.get('avg_reward', 0):.2f}\n"

        return stats

    def interactive_session(self):
        """
        Start an interactive session where human can communicate with agent

        Commands:
        - status: Show agent status
        - knowledge: Show knowledge base
        - goals: Show current goals
        - ask <question>: Ask agent a question
        - tell <information>: Tell agent something
        - approve <request_id>: Approve a request
        - quit: Exit
        """

        print("\n" + "="*70)
        print("AEGIS Interactive Session")
        print("Type 'help' for commands")
        print("="*70 + "\n")

        while True:
            try:
                user_input = input("\n>>> ").strip()

                if not user_input:
                    continue

                if user_input == 'quit' or user_input == 'exit':
                    print("Ending session...")
                    break

                elif user_input == 'help':
                    self._print_help()

                elif user_input == 'start':
                    if self.autonomous_mode:
                        print("\n‚ö† Already running in autonomous mode")
                    else:
                        print("\nüöÄ Starting autonomous operation...")
                        print("Agent will continuously think, learn, and evolve")
                        print("Low-risk improvements will be auto-approved")
                        print("Press Ctrl+C to return to interactive mode\n")
                        try:
                            self.start_autonomous_operation()
                        except KeyboardInterrupt:
                            print("\n\n‚è∏ Autonomous operation paused")
                            self.autonomous_mode = False

                elif user_input == 'stop':
                    if not self.autonomous_mode:
                        print("\n‚ö† Not currently running in autonomous mode")
                    else:
                        print("\n‚èπ Stopping autonomous operation...")
                        self.autonomous_mode = False
                        print("‚úì Stopped")

                elif user_input == 'refresh':
                    print("\nüîÑ Refreshing agent's view of current architecture...\n")

                    # Show what the AGENT is actually using
                    print("Agent's Active Reasoning Engine:")
                    if hasattr(self.agent, 'reasoning_engine') and self.agent.reasoning_engine:
                        agent_engine = self.agent.reasoning_engine
                        print(f"  Type: {type(agent_engine).__name__}")

                        # If using pretrained LLM
                        if hasattr(self.agent, 'use_pretrained_llm') and self.agent.use_pretrained_llm:
                            print(f"  Pretrained LLM: {agent_engine.model_name}")
                            print(f"  Parameters: {sum(p.numel() for p in agent_engine.model.parameters()):,}")
                            print(f"\n  Note: Agent uses pretrained LLM for decisions")
                            print(f"        Base HRM is available but not used by agent")

                    # Show base HRM architecture (for evolution)
                    print("\nBase HRM Architecture (for evolution):")
                    if hasattr(self, 'reasoning_engine'):
                        actual_params = sum(p.numel() for p in self.reasoning_engine.parameters())
                        print(f"  Type: {type(self.reasoning_engine).__name__}")
                        print(f"  Total parameters: {actual_params:,}")

                        # For HRM models, show layer counts
                        if hasattr(self.reasoning_engine, 'high_level'):
                            if hasattr(self.reasoning_engine.high_level, 'blocks'):
                                actual_high = len(self.reasoning_engine.high_level.blocks)
                                actual_low = len(self.reasoning_engine.low_level.blocks)

                                print(f"  High-level layers: {actual_high}")
                                print(f"  Low-level layers: {actual_low}")

                                # Update config to match reality
                                if self.config.high_level_layers != actual_high:
                                    print(f"\n  ‚ö† Config mismatch detected!")
                                    print(f"     Config says: {self.config.high_level_layers} high-level layers")
                                    print(f"     Model has: {actual_high} high-level layers")
                                    print(f"     Updating config to match model...")

                                    self.config.high_level_layers = actual_high
                                    self.config.low_level_layers = actual_low

                                    print(f"  ‚úì Config updated")
                            else:
                                print("  (Layer structure not accessible)")
                        else:
                            print("  (Not an HRM model)")

                    # Clear stale proposal tracking
                    old_count = len(self.proposed_improvements)
                    self.proposed_improvements.clear()
                    self.recent_proposals.clear()

                    print(f"\n  Cleared {old_count} tracked proposals")

                    # Re-sync with pending requests
                    pending = self.approval_manager.get_pending_requests()
                    for req in pending:
                        if "Add Hierarchical" in req.title:
                            self.proposed_improvements.add("architecture:Add Hierarchical Attention Layer")
                        elif "Adaptive Learning" in req.title:
                            self.proposed_improvements.add("learning:Enable Adaptive Learning Rate")
                        elif "Attention Mechanism" in req.title:
                            self.proposed_improvements.add("reasoning:Modify Attention Mechanism")

                    print(f"  Re-synced {len(self.proposed_improvements)} pending proposals")
                    print("\n‚úì Refresh complete")

                elif user_input == 'status':
                    print("\n" + self._format_operation_stats())

                elif user_input == 'training':
                    print("\n" + "="*70)
                    print("GPT-2 TRAINING STATUS")
                    print("="*70)
                    print(self._format_training_stats())

                    # Additional detailed info
                    if self.training_phase == 1:
                        collector_stats = self.data_collector.get_stats()
                        print(f"\nData Collection Details:")
                        print(f"  Total examples: {collector_stats['total_examples']}")
                        print(f"  Action distribution:")
                        for action, count in collector_stats.get('action_distribution', {}).items():
                            print(f"    {action}: {count}")
                        print(f"  Avg knowledge per example: {collector_stats['avg_knowledge_per_example']:.2f}")
                        print(f"\n  Next milestone: {1000000 - collector_stats['total_examples']:,} examples until HRM training")

                    elif self.training_phase == 2:
                        print(f"\nGPT-2 LoRA Fine-Tuning in Progress...")
                        print(f"  Training on collected agent decisions")
                        print(f"  LoRA adapters: ~800K trainable parameters (0.65% of GPT-2)")

                    elif self.training_phase == 3 and self.rl_trainer:
                        rl_stats = self.rl_trainer.get_stats()
                        print(f"\nRL Fine-Tuning Details:")
                        print(f"  Experience buffer size: {rl_stats['buffer_size']}")
                        print(f"  Total policy updates: {rl_stats['total_updates']}")
                        print(f"  Cumulative reward: {rl_stats['cumulative_reward']:.2f}")
                        print(f"  Average reward per update: {rl_stats['avg_reward']:.2f}")

                    print("="*70)

                elif user_input == 'knowledge':
                    stats = self.knowledge_system.knowledge_base.get_stats()
                    print(f"\nKnowledge Base: {stats}")

                elif user_input == 'goals':
                    goals = self.agent.goal_generator.active_goals
                    completed = self.agent.goal_generator.completed_goals

                    print(f"\n{'='*70}")
                    print(f"ACTIVE GOALS ({len(goals)})")
                    print(f"{'='*70}")

                    if not goals:
                        print("  No active goals")
                    else:
                        for i, goal in enumerate(goals, 1):
                            status = "‚úì" if goal.completed else "‚Üí"
                            print(f"\n  {status} Goal {i}: {goal.description}")
                            print(f"     Type: {goal.goal_type.value}")
                            print(f"     Progress: {goal.progress*100:.0f}%")
                            print(f"     Priority: {goal.priority:.2f}")
                            print(f"     Motivation: {goal.motivation}")
                            if goal.actions_taken:
                                print(f"     Actions Taken: {len(goal.actions_taken)}")

                    if completed:
                        print(f"\n{'='*70}")
                        print(f"COMPLETED GOALS ({len(completed)})")
                        print(f"{'='*70}")
                        for i, goal in enumerate(completed[:5], 1):  # Show last 5
                            print(f"  ‚úì {goal.description}")

                elif user_input.startswith('ask '):
                    question = user_input[4:]
                    # Agent generates answer (would use reasoning engine)
                    print(f"\nAgent: I'll think about that and search for information...")
                    result = self._web_search(question)
                    print(f"Agent: {result}")

                elif user_input.startswith('tell '):
                    information = user_input[5:]
                    topic = self._extract_topic(information)
                    item_id = self.knowledge_system.add_human_knowledge(
                        content=information,
                        topic=topic
                    )
                    print(f"\nAgent: Thank you! I've added that to my knowledge base (ID: {item_id})")

                elif user_input == 'pending' or user_input == 'approvals':
                    pending = self.approval_manager.get_pending_requests()

                    print(f"\n{'='*70}")
                    print(f"PENDING APPROVAL REQUESTS ({len(pending)})")
                    print(f"{'='*70}")

                    if not pending:
                        print("  No pending requests")
                    else:
                        for i, req in enumerate(pending, 1):
                            print(f"\n  Request {i}:")
                            print(f"     ID: {req.request_id}")
                            print(f"     Type: {req.change_type.value}")
                            print(f"     Title: {req.title}")
                            print(f"     Description: {req.description}")
                            print(f"     Created: {req.created_at}")
                            print(f"     Expires: {req.expires_at}")
                            print(f"\n     To approve: approve {req.request_id}")

                elif user_input.startswith('approve '):
                    request_id = user_input[8:].strip()
                    success = self.approval_manager.approve_request(
                        request_id=request_id,
                        reviewer_name="Human Operator",
                        approval_code="interactive_session_approval",
                        notes="Approved via interactive session"
                    )
                    if success:
                        print(f"\n‚úì Request {request_id} approved")
                    else:
                        print(f"\n‚úó Could not approve request {request_id}")

                elif user_input.startswith('reject '):
                    request_id = user_input[7:].strip()
                    reason = input("Rejection reason: ").strip()
                    success = self.approval_manager.reject_request(
                        request_id=request_id,
                        reviewer_name="Human Operator",
                        reason=reason
                    )
                    if success:
                        print(f"\n‚úì Request {request_id} rejected")
                    else:
                        print(f"\n‚úó Could not reject request {request_id}")

                elif user_input == 'clear_tracking':
                    print("\nüóëÔ∏è  Clearing duplicate tracking...")
                    old_count = len(self.proposed_improvements)
                    self.proposed_improvements.clear()
                    self.recent_proposals.clear()
                    print(f"  ‚úì Cleared {old_count} tracked proposals")
                    print(f"  Agent will now re-propose previously suggested improvements")

                elif user_input == 'cleanup' or user_input == 'clear':
                    print("\nüßπ Cleaning up duplicate pending requests...")

                    pending = self.approval_manager.get_pending_requests()

                    # Group by title
                    from collections import defaultdict
                    by_title = defaultdict(list)
                    for req in pending:
                        by_title[req.title].append(req)

                    # Reject duplicates, keep oldest
                    duplicates_removed = 0
                    for title, requests in by_title.items():
                        if len(requests) > 1:
                            print(f"\n  Found {len(requests)} duplicate requests: {title}")
                            # Sort by creation time, keep oldest
                            sorted_reqs = sorted(requests, key=lambda r: r.created_at)
                            print(f"  Keeping oldest: {sorted_reqs[0].request_id}")

                            # Reject duplicates
                            for req in sorted_reqs[1:]:
                                self.approval_manager.reject_request(
                                    request_id=req.request_id,
                                    reviewer_name="System Cleanup",
                                    reason="Duplicate request - keeping oldest"
                                )
                                duplicates_removed += 1

                    print(f"\n‚úì Removed {duplicates_removed} duplicate requests")

                    # Update tracking
                    self.proposed_improvements.clear()
                    for req in self.approval_manager.get_pending_requests():
                        # Extract aspect and name from title
                        if "Add Hierarchical" in req.title:
                            self.proposed_improvements.add("architecture:Add Hierarchical Attention Layer")
                        elif "Adaptive Learning" in req.title:
                            self.proposed_improvements.add("learning:Enable Adaptive Learning Rate")
                        elif "Attention Mechanism" in req.title:
                            self.proposed_improvements.add("reasoning:Modify Attention Mechanism")

                elif user_input == 'think':
                    print("\nAgent is thinking...")
                    action = self.agent.think()
                    print(f"  Decision: {action['action']}")
                    if action['action'] != 'idle':
                        for key, value in action.items():
                            if key != 'action':
                                print(f"  {key}: {value}")

                        execute = input("\nExecute this action? (y/n): ").strip().lower()
                        if execute == 'y':
                            result = self.agent.execute_action(action)
                            print(f"\n‚úì Action executed: {result.get('status', 'unknown')}")

                else:
                    print("\nUnknown command. Type 'help' for available commands.")

            except KeyboardInterrupt:
                print("\n\nSession interrupted. Type 'quit' to exit.")
            except Exception as e:
                print(f"\nError: {e}")

    def _print_help(self):
        """Print help information"""

        print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    AEGIS INTERACTIVE COMMANDS                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üöÄ AUTONOMOUS OPERATION:
  start               - Start continuous autonomous operation
  stop                - Stop autonomous operation

üìä INFORMATION:
  status              - Show system and agent status
  refresh             - Refresh agent's view of current architecture
  training            - Show detailed HRM training status and progress
  knowledge           - Show knowledge base statistics
  goals               - Show agent's current goals (detailed)
  pending             - Show pending approval requests

üí≠ INTERACTION:
  think               - Make agent think and show decision (manual execution)
  ask <question>      - Ask the agent a question
  tell <information>  - Tell the agent something

‚úÖ APPROVALS:
  approve <id>        - Approve a pending request
  reject <id>         - Reject a pending request
  cleanup             - Remove duplicate pending requests
  clear_tracking      - Clear duplicate tracking (allows re-proposing)

‚ÑπÔ∏è  OTHER:
  help                - Show this help message
  quit                - Exit interactive session

EXAMPLES:
  >>> start                    (continuous autonomous mode)
  >>> goals
  >>> pending
  >>> approve 4f7fe7e1-e6f3-40a2-b174-0cc3e9af0841
  >>> refresh                  (sync config with model)
  >>> think
  >>> ask What is neural architecture search?
""")


def main():
    """Main entry point for autonomous AEGIS"""

    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                              ‚ïë
‚ïë     AEGIS: Adaptive Evolutionary General Intelligence       ‚ïë
‚ïë                   Autonomous Mode                            ‚ïë
‚ïë                                                              ‚ïë
‚ïë  A self-directed AGI that:                                   ‚ïë
‚ïë  ‚Ä¢ Sets its own goals based on curiosity                     ‚ïë
‚ïë  ‚Ä¢ Asks questions to learn                                   ‚ïë
‚ïë  ‚Ä¢ Searches for knowledge autonomously                       ‚ïë
‚ïë  ‚Ä¢ Proposes self-improvements                                ‚ïë
‚ïë  ‚Ä¢ Evolves with human supervision                            ‚ïë
‚ïë                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")

    # Create autonomous AEGIS
    config = AEGISConfig(
        vocab_size=1000,
        d_model=256,
        high_level_layers=4,
        low_level_layers=2,
        population_size=5,
        require_approval_for_deployment=True,
        require_approval_for_code_gen=True,
        auto_freeze_on_emergence=True
    )

    aegis = AutonomousAEGIS(config)

    # Interactive session
    aegis.interactive_session()

    print("\nThank you for using AEGIS!")


if __name__ == "__main__":
    main()
